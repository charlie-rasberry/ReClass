{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b7cfa1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "667df51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========TAGGED ORIGINAL========\n",
      "                                              review  rating  word_count  \\\n",
      "0      their have many problem but also best service       5           8   \n",
      "1           it's excellent i loved it thank you uber       5           8   \n",
      "2    it does the job as it should be, in a nice way!       5          12   \n",
      "3  i support my family members with the help of uber       5          10   \n",
      "4         it's good bt it is only.for 1 man or woman       5          10   \n",
      "\n",
      "   tagged feature_request bug_report   aspect aspect_sentiment  \n",
      "0       1              No         No  Service         Positive  \n",
      "1       1              No         No  General         Positive  \n",
      "2       1              No         No  General         Positive  \n",
      "3       1              No         No  General         Positive  \n",
      "4       1             Yes         No  General         Positive  \n",
      "\n",
      "========TAGGED BOOSTED========\n",
      "                                              review  word_count  rating  \\\n",
      "0  \"\"*the worst customer care and worst transport...          51     NaN   \n",
      "1                guy was excellent give him q raise!           7     1.0   \n",
      "2  \"\"poor service provider company, i have an err...          50     NaN   \n",
      "3  \"\"this app did not let me schedule a ride for ...          99     NaN   \n",
      "4  \"\"worst app.always high prices and drivers alw...          25     NaN   \n",
      "\n",
      "   tagged feature_request bug_report   aspect aspect_sentiment  \n",
      "0       1              No         No  Service         Negative  \n",
      "1       1              No         No   Driver         Positive  \n",
      "2       1              No        Yes      App         Negative  \n",
      "3       1              No        Yes      App         Negative  \n",
      "4       1              No         No  Pricing         Negative  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "tagged_orignal_df = pd.read_csv('../data/uber_reviews_taggedOriginal.csv')\n",
    "print(f\"\\n========TAGGED ORIGINAL========\")\n",
    "print(tagged_orignal_df.head())\n",
    "tagged_boosted_df = pd.read_csv('../data/uber_reviews_taggedBoosted.csv')\n",
    "print(f\"\\n========TAGGED BOOSTED========\")\n",
    "print(tagged_boosted_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5cf6b62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows was tagged_orignal_df: 0\n",
      "Number of duplicate rows was tagged_boosted_df: 3\n",
      "(These were removed in the next cell)\n",
      "\n",
      "Current duplicates: \n",
      "Number of duplicate rows in tagged_orignal_df: 0\n",
      "Number of duplicate rows in tagged_boosted_df: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of duplicate rows was tagged_orignal_df: 0\")\n",
    "print(\"Number of duplicate rows was tagged_boosted_df: 3\\n(These were removed in the next cell)\\n\\nCurrent duplicates: \")\n",
    "\n",
    "\n",
    "duplicates = tagged_orignal_df.duplicated()\n",
    "print(f\"Number of duplicate rows in tagged_orignal_df: {duplicates.sum()}\")\n",
    "\n",
    "duplicates = tagged_boosted_df.duplicated()\n",
    "print(f\"Number of duplicate rows in tagged_boosted_df: {duplicates.sum()}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0ad690d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_orignal_df = tagged_orignal_df.drop_duplicates()\n",
    "tagged_boosted_df = tagged_boosted_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a89755b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of exact duplicates in tagged_boosted_df: 0\n",
      "Number of exact duplicates in tagged_orignal_df: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of exact duplicates in tagged_boosted_df:\", tagged_boosted_df[tagged_boosted_df.duplicated(subset=['review'])].shape[0])\n",
    "print(\"Number of exact duplicates in tagged_orignal_df:\", tagged_orignal_df[tagged_orignal_df.duplicated(subset=['review'])].shape[0])\n",
    "\n",
    "tagged_boosted_df = tagged_boosted_df.drop_duplicates(subset=['review']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f7d58696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique reviews: 4989\n",
      "Rows involved in exact duplication: 0\n",
      "Distinct reviews that are duplicated: 0\n"
     ]
    }
   ],
   "source": [
    "# How many unique reviews remain if you were to drop exact dupes\n",
    "print(\"Unique reviews:\", tagged_boosted_df['review'].nunique())\n",
    "\n",
    "# How many duplicate pairs exist (not rows, but pairs)\n",
    "exact_dupe_rows = tagged_boosted_df[tagged_boosted_df.duplicated(subset=['review'], keep=False)]\n",
    "print(\"Rows involved in exact duplication:\", exact_dupe_rows.shape[0])\n",
    "print(\"Distinct reviews that are duplicated:\", tagged_boosted_df.duplicated(subset=['review']).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "596d0f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosted sample:\n",
      "0    \"\"*the worst customer care and worst transport...\n",
      "1                  guy was excellent give him q raise!\n",
      "2    \"\"poor service provider company, i have an err...\n",
      "3    \"\"this app did not let me schedule a ride for ...\n",
      "4    \"\"worst app.always high prices and drivers alw...\n",
      "5     \"\"ac not worked, driver call the whole journey\"\"\n",
      "6                                  i think ola is best\n",
      "7    \"\"when it comes to rides, they're great, but w...\n",
      "8    am using note 3 samsung. every time this app n...\n",
      "9    ÿ≠ÿ±ÿßŸÖŸäÿ© ÿ≤ŸÅÿ™ ÿ∂Ÿäÿπ ŸàŸÇÿ™Ÿâ ŸàÿßÿÆÿØ ÿßŸÑÿ∑ÿ±ŸäŸÇ ÿßŸÑÿ∑ŸàŸäŸÑ ÿπÿ¥ÿßŸÜ Ÿäÿ∂...\n",
      "Name: review, dtype: object\n",
      "\n",
      "Original sample:\n",
      "0        their have many problem but also best service\n",
      "1             it's excellent i loved it thank you uber\n",
      "2      it does the job as it should be, in a nice way!\n",
      "3    i support my family members with the help of uber\n",
      "4           it's good bt it is only.for 1 man or woman\n",
      "5                   easy to use and much more accurate\n",
      "6         good and comfortable drive also save for us.\n",
      "7                            best ride for dhaka city.\n",
      "8              friendly person thanks for your support\n",
      "9         a very good conversationalist! good driving.\n",
      "Name: review, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Boosted sample:\")\n",
    "print(tagged_boosted_df['review'].head(10))\n",
    "print(\"\\nOriginal sample:\")\n",
    "print(tagged_orignal_df['review'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e6b97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosted - leading quote artifacts: 29\n",
      "Original - leading quote artifacts: 4\n",
      "Boosted - doubled internal quotes: 29\n",
      "Original - doubled internal quotes: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Boosted - leading quote artifacts:\", tagged_boosted_df[tagged_boosted_df['review'].str.startswith('\"')].shape[0])\n",
    "print(\"Original - leading quote artifacts:\", tagged_orignal_df[tagged_orignal_df['review'].str.startswith('\"')].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "926849ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosted - leading quote artifacts: 29\n",
      "Original - leading quote artifacts: 4\n",
      "Boosted - doubled internal quotes: 0\n",
      "Original - doubled internal quotes: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Boosted - leading quote artifacts:\", tagged_boosted_df[tagged_boosted_df['review'].str.startswith('\"')].shape[0])\n",
    "print(\"Original - leading quote artifacts:\", tagged_orignal_df[tagged_orignal_df['review'].str.startswith('\"')].shape[0])\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_quote_artifacts(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    # Strip any number of leading/trailing quote chars and whitespace\n",
    "    text = text.strip('\" \\t')\n",
    "    # Collapse runs of 2+ quotes down to a single quote (for internal ones like \"\"\"\"\"\"\"\")\n",
    "    text = re.sub(r'\"{2,}', '\"', text)\n",
    "    # Clean up any quotes now left dangling at edges after internal collapse\n",
    "    text = text.strip('\" ')\n",
    "    return text\n",
    "\n",
    "tagged_boosted_df['review'] = tagged_boosted_df['review'].apply(clean_quote_artifacts)\n",
    "tagged_orignal_df['review'] = tagged_orignal_df['review'].apply(clean_quote_artifacts)\n",
    "\n",
    "print(\"Boosted - doubled internal quotes:\", tagged_boosted_df[tagged_boosted_df['review'].str.contains('\"{2,}', regex=True)].shape[0])\n",
    "print(\"Original - doubled internal quotes:\", tagged_orignal_df[tagged_orignal_df['review'].str.contains('\"{2,}', regex=True)].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "96c0520b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosted:  4,989 rows\n",
      "Original: 4,999 rows\n",
      "Boosted nulls:\n",
      " review               0\n",
      "word_count           0\n",
      "rating              84\n",
      "tagged               0\n",
      "feature_request      0\n",
      "bug_report           0\n",
      "aspect               0\n",
      "aspect_sentiment     0\n",
      "dtype: int64\n",
      "\n",
      "Original nulls:\n",
      " review              0\n",
      "rating              0\n",
      "word_count          0\n",
      "tagged              0\n",
      "feature_request     0\n",
      "bug_report          0\n",
      "aspect              0\n",
      "aspect_sentiment    0\n",
      "dtype: int64\n",
      "Boosted - empty reviews: 0\n",
      "Original - empty reviews: 0\n",
      "Boosted - leading quote artifacts: 0\n",
      "Original - leading quote artifacts: 0\n",
      "Boosted - doubled internal quotes: 0\n",
      "Original - doubled internal quotes: 0\n",
      "Boosted columns match: True\n",
      "Original columns match: False\n",
      "[\"*the worst customer care and worst transportation app in sri lanka because of it's staff service. *uber service in sri lanka is a shame to uber technologies, inc. *i expected better service because it's international service but in sri lanka it's better to use local transportation services than using uber! uber!\"\n",
      " 'worst customer care service they app stole my 140rs and not even replying'\n",
      " 'worst customer care service.cab service are good and needs to pay attention to the dynamic pricing. i feel ola is better in most of the cases. feb16 2019: bill duplicate and charged twice. customer care is so patatic and request for all the statements with no solution. i feel that there is no mechanism to track if the payment is successful or not. this is happening only with uber and second of such instance. i seen most of the online site does nice error handling and instantly give updates.'\n",
      " \"i hate this app. not working from 10 days and worst customer care service. no one is resolving. one of my uber account is disabled. and there unable to provide any reason and the other account. it says error processing your request. i tried to open another account. it is not even registring. the worst customer care service i have seen in my life is uber. 1 don't want to give even that 1 star to u. but it is the minimum. can't helpüò†üò†üò†\"\n",
      " \"worst customer care. worst service sometimes. the cab arrives 1 hour late yet they charge the cancellation amount. when complaint is made to customer care, they don't speak proper english or they were unable to understand. they don't listen to understand what the customer says. they are very keen on closing the issue and not interested in caring for the customer.\"\n",
      " \"you have the worst customer care support. you simply waste people's time without taking any option. rapido is far better atleast they have a helpline number where they call. i asked your customer support to call me on phone,but you always insist on messaging and keep on asking same thing again and again. i would give you minus review had there been options.\"\n",
      " \"worst customer care support possible. i lost my phone in uber and i couldn't contact anyone to help me out. only if uber was caring enough for their customers i would have not lost my phone and got it back!\"\n",
      " 'they have the worst payment system and worst customer care service too. i had a 50rs outstanding and 50 rs credits. but it was not balancing those while booking and didnt let me book. so i paid through credit card 50 rs. that 50 rs is nowhere in their system as well as got deducted from my account. even irctc has much better refund system than theirs. asking the customer care you get same automated reply irrespective of whatever you ask. no telephone customer care. what are they running, a local grocery shop!']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Boosted:  {len(tagged_boosted_df):,} rows\")\n",
    "print(f\"Original: {len(tagged_orignal_df):,} rows\")\n",
    "print(\"Boosted nulls:\\n\", tagged_boosted_df.isnull().sum())\n",
    "print(\"\\nOriginal nulls:\\n\", tagged_orignal_df.isnull().sum())\n",
    "print(\"Boosted - empty reviews:\", tagged_boosted_df['review'].str.strip().eq('').sum())\n",
    "print(\"Original - empty reviews:\", tagged_orignal_df['review'].str.strip().eq('').sum())\n",
    "print(\"Boosted - leading quote artifacts:\", tagged_boosted_df[tagged_boosted_df['review'].str.startswith('\"')].shape[0])\n",
    "print(\"Original - leading quote artifacts:\", tagged_orignal_df[tagged_orignal_df['review'].str.startswith('\"')].shape[0])\n",
    "print(\"Boosted - doubled internal quotes:\", tagged_boosted_df[tagged_boosted_df['review'].str.contains('\"{2,}', regex=True)].shape[0])\n",
    "print(\"Original - doubled internal quotes:\", tagged_orignal_df[tagged_orignal_df['review'].str.contains('\"{2,}', regex=True)].shape[0])\n",
    "expected_cols = ['review', 'word_count', 'rating', 'tagged', 'feature_request', 'bug_report', 'aspect', 'aspect_sentiment']\n",
    "print(\"Boosted columns match:\", list(tagged_boosted_df.columns) == expected_cols)\n",
    "print(\"Original columns match:\", list(tagged_orignal_df.columns) == expected_cols)\n",
    "print(tagged_boosted_df[tagged_boosted_df['review'].str.contains('worst customer care', na=False)]['review'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27362604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8b76f032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspect_sentiment\n",
      "Negative    75\n",
      "Positive     9\n",
      "Name: count, dtype: int64\n",
      "aspect\n",
      "Driver     23\n",
      "App        21\n",
      "Service    20\n",
      "Payment    12\n",
      "Pricing     7\n",
      "General     1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Are the 84 null ratings clustered in a particular aspect/sentiment or spread across?\n",
    "print(tagged_boosted_df[tagged_boosted_df['rating'].isnull()]['aspect_sentiment'].value_counts())\n",
    "print(tagged_boosted_df[tagged_boosted_df['rating'].isnull()]['aspect'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aade3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosted saved:  4,989 rows\n",
      "Original saved: 4,999 rows\n"
     ]
    }
   ],
   "source": [
    "tagged_boosted_df.to_csv('tagged_boosted_cleaned.csv', index=False)\n",
    "tagged_orignal_df.to_csv('tagged_original_cleaned.csv', index=False)\n",
    "# I can confirm, the saved files have no malformed rows/number of columns is correct\n",
    "print(f\"Boosted saved:  {len(tagged_boosted_df):,} rows\")\n",
    "print(f\"Original saved: {len(tagged_orignal_df):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7e814adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========== Boosted ===========\n",
      "bug_report          0\n",
      "feature_request     0\n",
      "aspect              0\n",
      "aspect_sentiment    0\n",
      "dtype: int64\n",
      "bug_report          int64\n",
      "feature_request     int64\n",
      "aspect              int64\n",
      "aspect_sentiment    int64\n",
      "dtype: object\n",
      "   bug_report  feature_request  aspect  aspect_sentiment\n",
      "0           0                0       5                 2\n",
      "1           0                0       1                 0\n",
      "2           1                0       0                 2\n",
      "\n",
      "=========== Original ===========\n",
      "bug_report          0\n",
      "feature_request     0\n",
      "aspect              0\n",
      "aspect_sentiment    0\n",
      "dtype: int64\n",
      "bug_report          int64\n",
      "feature_request     int64\n",
      "aspect              int64\n",
      "aspect_sentiment    int64\n",
      "dtype: object\n",
      "   bug_report  feature_request  aspect  aspect_sentiment\n",
      "0           0                0       5                 0\n",
      "1           0                0       2                 0\n",
      "2           0                0       2                 0\n"
     ]
    }
   ],
   "source": [
    "# Now I need to convert/map yes / no to integers, same for all  tasks.\n",
    "# How did everything run earlier? there was a spelling mistake but everything looked fine\n",
    "tagged_original_df = pd.read_csv('../data/tagged_original_cleaned.csv')\n",
    "tagged_boosted_df = pd.read_csv('../data/tagged_boosted_cleaned.csv')\n",
    "# mappings\n",
    "binary_map = {'Yes': 1, 'No': 0}\n",
    "aspect_map = {'App': 0, 'Driver': 1, 'General': 2, 'Payment': 3, 'Pricing': 4, 'Service': 5}\n",
    "sentiment_map = {'Positive': 0, 'Neutral': 1, 'Negative': 2}\n",
    "\n",
    "tagged_original_df = pd.read_csv('../data/tagged_original_cleaned.csv')\n",
    "tagged_boosted_df  = pd.read_csv('../data/tagged_boosted_cleaned.csv')\n",
    "\n",
    "for df in [tagged_boosted_df, tagged_original_df]:\n",
    "    df['bug_report']       = df['bug_report'].map(binary_map)\n",
    "    df['feature_request']  = df['feature_request'].map(binary_map)\n",
    "    df['aspect']           = df['aspect'].map(aspect_map)\n",
    "    df['aspect_sentiment'] = df['aspect_sentiment'].map(sentiment_map)\n",
    "    \n",
    "for df, name in [(tagged_boosted_df, 'Boosted'), (tagged_original_df, 'Original')]:\n",
    "\n",
    "    # Verification after mapping\n",
    "    print(f\"\\n=========== {name} ===========\")\n",
    "    print(df[['bug_report', 'feature_request', 'aspect', 'aspect_sentiment']].isnull().sum())\n",
    "    print(df[['bug_report', 'feature_request', 'aspect', 'aspect_sentiment']].dtypes)\n",
    "    print(df[['bug_report', 'feature_request', 'aspect', 'aspect_sentiment']].head(3))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "093432db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosted saved:  4,989 rows\n",
      "Original saved: 4,999 rows\n"
     ]
    }
   ],
   "source": [
    "tagged_boosted_df.to_csv('../data/tagged_boosted_cleaned.csv', index=False)\n",
    "tagged_original_df.to_csv('../data/tagged_original_cleaned.csv', index=False)\n",
    "\n",
    "print(f\"Boosted saved:  {len(tagged_boosted_df):,} rows\")\n",
    "print(f\"Original saved: {len(tagged_original_df):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8fb12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "[0 1]\n",
      "\n",
      "../data/tagged_boosted_cleaned.csv\n",
      "  Expected columns: 8\n",
      "  Malformed rows:   0\n",
      "\n",
      "../data/tagged_original_cleaned.csv\n",
      "  Expected columns: 8\n",
      "  Malformed rows:   0\n"
     ]
    }
   ],
   "source": [
    "print(tagged_boosted_df['bug_report'].unique())\n",
    "print(tagged_original_df['bug_report'].unique())\n",
    "\n",
    "# CSV structure checks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bc252c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I can finally split into train/test sets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562bf9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 3,492 (0.6999398677089597)\n",
      "Val:   748   (0.149929845660453)\n",
      "Test:  749  (0.15013028663058728)\n",
      "Train: 3,499 (0.6999399879975995)\n",
      "Val:   750   (0.15003000600120023)\n",
      "Test:  750  (0.15003000600120023)\n"
     ]
    }
   ],
   "source": [
    "def split(df, name, seed=67):\n",
    "    train, temp = train_test_split(df, test_size=0.30, random_state=seed, stratify=df['aspect_sentiment'])\n",
    "    val, test = train_test_split(temp, test_size=0.50, random_state=seed, stratify=temp['aspect_sentiment'])\n",
    "\n",
    "    print(f\"Train: {len(train):,} ({len(train)/len(df)})\")\n",
    "    print(f\"Val:   {len(val):,}   ({len(val)/len(df)})\")\n",
    "    print(f\"Test:  {len(test):,}  ({len(test)/len(df)})\")\n",
    "    return train, val, test\n",
    "\n",
    "tagged_boosted_df = pd.read_csv('../data/tagged_boosted_cleaned.csv')\n",
    "tagged_original_df = pd.read_csv('../data/tagged_original_cleaned.csv')\n",
    "\n",
    "train_b, val_b, test_b = split(tagged_boosted_df, 'Boosted')\n",
    "train_o, val_o, test_o = split(tagged_original_df, 'Original')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b3595063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========== Boosted Train ===========\n",
      "bug_report:\n",
      " bug_report\n",
      "0    0.648053\n",
      "1    0.351947\n",
      "Name: proportion, dtype: float64\n",
      "feature_request:\n",
      " feature_request\n",
      "0    0.909507\n",
      "1    0.090493\n",
      "Name: proportion, dtype: float64\n",
      "aspect:\n",
      " aspect\n",
      "0    0.285223\n",
      "1    0.251432\n",
      "5    0.154066\n",
      "2    0.118557\n",
      "3    0.108820\n",
      "4    0.081901\n",
      "Name: proportion, dtype: float64\n",
      "aspect_sentiment:\n",
      " aspect_sentiment\n",
      "2    0.808706\n",
      "0    0.182703\n",
      "1    0.008591\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "=========== Boosted Val ===========\n",
      "bug_report:\n",
      " bug_report\n",
      "0    0.649733\n",
      "1    0.350267\n",
      "Name: proportion, dtype: float64\n",
      "feature_request:\n",
      " feature_request\n",
      "0    0.895722\n",
      "1    0.104278\n",
      "Name: proportion, dtype: float64\n",
      "aspect:\n",
      " aspect\n",
      "0    0.283422\n",
      "1    0.236631\n",
      "5    0.168449\n",
      "2    0.112299\n",
      "3    0.108289\n",
      "4    0.090909\n",
      "Name: proportion, dtype: float64\n",
      "aspect_sentiment:\n",
      " aspect_sentiment\n",
      "2    0.808824\n",
      "0    0.183155\n",
      "1    0.008021\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "=========== Boosted Test ===========\n",
      "bug_report:\n",
      " bug_report\n",
      "0    0.670227\n",
      "1    0.329773\n",
      "Name: proportion, dtype: float64\n",
      "feature_request:\n",
      " feature_request\n",
      "0    0.895861\n",
      "1    0.104139\n",
      "Name: proportion, dtype: float64\n",
      "aspect:\n",
      " aspect\n",
      "0    0.269693\n",
      "1    0.244326\n",
      "5    0.146862\n",
      "2    0.117490\n",
      "3    0.116155\n",
      "4    0.105474\n",
      "Name: proportion, dtype: float64\n",
      "aspect_sentiment:\n",
      " aspect_sentiment\n",
      "2    0.809079\n",
      "0    0.182911\n",
      "1    0.008011\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "=========== Original Train ===========\n",
      "bug_report:\n",
      " bug_report\n",
      "0    0.818805\n",
      "1    0.181195\n",
      "Name: proportion, dtype: float64\n",
      "feature_request:\n",
      " feature_request\n",
      "0    0.84367\n",
      "1    0.15633\n",
      "Name: proportion, dtype: float64\n",
      "aspect:\n",
      " aspect\n",
      "0    0.293512\n",
      "5    0.214347\n",
      "1    0.192626\n",
      "2    0.157759\n",
      "4    0.096599\n",
      "3    0.045156\n",
      "Name: proportion, dtype: float64\n",
      "aspect_sentiment:\n",
      " aspect_sentiment\n",
      "0    0.514433\n",
      "2    0.433267\n",
      "1    0.052301\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "=========== Original Val ===========\n",
      "bug_report:\n",
      " bug_report\n",
      "0    0.805333\n",
      "1    0.194667\n",
      "Name: proportion, dtype: float64\n",
      "feature_request:\n",
      " feature_request\n",
      "0    0.849333\n",
      "1    0.150667\n",
      "Name: proportion, dtype: float64\n",
      "aspect:\n",
      " aspect\n",
      "0    0.282667\n",
      "5    0.225333\n",
      "1    0.192000\n",
      "2    0.180000\n",
      "4    0.076000\n",
      "3    0.044000\n",
      "Name: proportion, dtype: float64\n",
      "aspect_sentiment:\n",
      " aspect_sentiment\n",
      "0    0.514667\n",
      "2    0.433333\n",
      "1    0.052000\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "=========== Original Test ===========\n",
      "bug_report:\n",
      " bug_report\n",
      "0    0.813333\n",
      "1    0.186667\n",
      "Name: proportion, dtype: float64\n",
      "feature_request:\n",
      " feature_request\n",
      "0    0.841333\n",
      "1    0.158667\n",
      "Name: proportion, dtype: float64\n",
      "aspect:\n",
      " aspect\n",
      "0    0.308000\n",
      "5    0.208000\n",
      "1    0.206667\n",
      "2    0.148000\n",
      "4    0.106667\n",
      "3    0.022667\n",
      "Name: proportion, dtype: float64\n",
      "aspect_sentiment:\n",
      " aspect_sentiment\n",
      "0    0.513333\n",
      "2    0.433333\n",
      "1    0.053333\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "splits = [\n",
    "    (train_b, 'Boosted Train'), (val_b, 'Boosted Val'), (test_b, 'Boosted Test'),\n",
    "    (train_o, 'Original Train'), (val_o, 'Original Val'), (test_o, 'Original Test')\n",
    "]\n",
    "\n",
    "for df, name in splits:\n",
    "    print(f\"\\n=========== {name} ===========\")\n",
    "    print(\"bug_report:\\n\", df['bug_report'].value_counts(normalize=True))\n",
    "    print(\"feature_request:\\n\", df['feature_request'].value_counts(normalize=True))\n",
    "    print(\"aspect:\\n\", df['aspect'].value_counts(normalize=True))\n",
    "    print(\"aspect_sentiment:\\n\", df['aspect_sentiment'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2a524db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 3,492 (0.6999398677089597)\n",
      "Val:   748   (0.149929845660453)\n",
      "Test:  749  (0.15013028663058728)\n",
      "Train: 3,499 (0.6999399879975995)\n",
      "Val:   750   (0.15003000600120023)\n",
      "Test:  750  (0.15003000600120023)\n"
     ]
    }
   ],
   "source": [
    "def split(df, name, seed=67):\n",
    "    composite = df['bug_report'].astype(str) + \"_\" + df['feature_request'].astype(str) + \"_\" + df['aspect_sentiment'].astype(str)\n",
    "    train, temp = train_test_split(df, test_size=0.30, random_state=seed, stratify=composite)\n",
    "    \n",
    "    composite_temp = temp['bug_report'].astype(str) + \"_\" + temp['feature_request'].astype(str) + \"_\" + temp['aspect_sentiment'].astype(str)\n",
    "    val, test = train_test_split(temp, test_size=0.50, random_state=seed, stratify=composite_temp)\n",
    "\n",
    "    print(f\"Train: {len(train):,} ({len(train)/len(df)})\")\n",
    "    print(f\"Val:   {len(val):,}   ({len(val)/len(df)})\")\n",
    "    print(f\"Test:  {len(test):,}  ({len(test)/len(df)})\")\n",
    "    return train, val, test\n",
    "\n",
    "tagged_boosted_df = pd.read_csv('../data/tagged_boosted_cleaned.csv')\n",
    "tagged_original_df = pd.read_csv('../data/tagged_original_cleaned.csv')\n",
    "\n",
    "train_b, val_b, test_b = split(tagged_boosted_df, 'Boosted')\n",
    "train_o, val_o, test_o = split(tagged_original_df, 'Original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bc486e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========== Boosted Train ===========\n",
      "bug_report:\n",
      " bug_report\n",
      "0    0.651489\n",
      "1    0.348511\n",
      "Name: proportion, dtype: float64\n",
      "feature_request:\n",
      " feature_request\n",
      "0    0.905212\n",
      "1    0.094788\n",
      "Name: proportion, dtype: float64\n",
      "aspect:\n",
      " aspect\n",
      "0    0.280928\n",
      "1    0.250286\n",
      "5    0.152348\n",
      "2    0.118270\n",
      "3    0.114261\n",
      "4    0.083906\n",
      "Name: proportion, dtype: float64\n",
      "aspect_sentiment:\n",
      " aspect_sentiment\n",
      "2    0.808706\n",
      "0    0.182990\n",
      "1    0.008305\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "=========== Boosted Val ===========\n",
      "bug_report:\n",
      " bug_report\n",
      "0    0.65107\n",
      "1    0.34893\n",
      "Name: proportion, dtype: float64\n",
      "feature_request:\n",
      " feature_request\n",
      "0    0.906417\n",
      "1    0.093583\n",
      "Name: proportion, dtype: float64\n",
      "aspect:\n",
      " aspect\n",
      "0    0.286096\n",
      "1    0.251337\n",
      "5    0.159091\n",
      "2    0.113636\n",
      "3    0.097594\n",
      "4    0.092246\n",
      "Name: proportion, dtype: float64\n",
      "aspect_sentiment:\n",
      " aspect_sentiment\n",
      "2    0.807487\n",
      "0    0.183155\n",
      "1    0.009358\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "=========== Boosted Test ===========\n",
      "bug_report:\n",
      " bug_report\n",
      "0    0.65287\n",
      "1    0.34713\n",
      "Name: proportion, dtype: float64\n",
      "feature_request:\n",
      " feature_request\n",
      "0    0.905207\n",
      "1    0.094793\n",
      "Name: proportion, dtype: float64\n",
      "aspect:\n",
      " aspect\n",
      "0    0.287049\n",
      "1    0.234980\n",
      "5    0.164219\n",
      "2    0.117490\n",
      "3    0.101469\n",
      "4    0.094793\n",
      "Name: proportion, dtype: float64\n",
      "aspect_sentiment:\n",
      " aspect_sentiment\n",
      "2    0.810414\n",
      "0    0.181575\n",
      "1    0.008011\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "=========== Original Train ===========\n",
      "bug_report:\n",
      " bug_report\n",
      "0    0.815947\n",
      "1    0.184053\n",
      "Name: proportion, dtype: float64\n",
      "feature_request:\n",
      " feature_request\n",
      "0    0.844241\n",
      "1    0.155759\n",
      "Name: proportion, dtype: float64\n",
      "aspect:\n",
      " aspect\n",
      "0    0.294941\n",
      "5    0.215490\n",
      "1    0.199486\n",
      "2    0.153472\n",
      "4    0.097456\n",
      "3    0.039154\n",
      "Name: proportion, dtype: float64\n",
      "aspect_sentiment:\n",
      " aspect_sentiment\n",
      "0    0.514433\n",
      "2    0.433267\n",
      "1    0.052301\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "=========== Original Val ===========\n",
      "bug_report:\n",
      " bug_report\n",
      "0    0.816\n",
      "1    0.184\n",
      "Name: proportion, dtype: float64\n",
      "feature_request:\n",
      " feature_request\n",
      "0    0.844\n",
      "1    0.156\n",
      "Name: proportion, dtype: float64\n",
      "aspect:\n",
      " aspect\n",
      "0    0.284000\n",
      "5    0.205333\n",
      "1    0.190667\n",
      "2    0.181333\n",
      "4    0.088000\n",
      "3    0.050667\n",
      "Name: proportion, dtype: float64\n",
      "aspect_sentiment:\n",
      " aspect_sentiment\n",
      "0    0.513333\n",
      "2    0.433333\n",
      "1    0.053333\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "=========== Original Test ===========\n",
      "bug_report:\n",
      " bug_report\n",
      "0    0.816\n",
      "1    0.184\n",
      "Name: proportion, dtype: float64\n",
      "feature_request:\n",
      " feature_request\n",
      "0    0.844\n",
      "1    0.156\n",
      "Name: proportion, dtype: float64\n",
      "aspect:\n",
      " aspect\n",
      "0    0.300000\n",
      "5    0.222667\n",
      "1    0.176000\n",
      "2    0.166667\n",
      "4    0.090667\n",
      "3    0.044000\n",
      "Name: proportion, dtype: float64\n",
      "aspect_sentiment:\n",
      " aspect_sentiment\n",
      "0    0.514667\n",
      "2    0.433333\n",
      "1    0.052000\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Using composite key\n",
    "\n",
    "splits = [\n",
    "    (train_b, 'Boosted Train'), (val_b, 'Boosted Val'), (test_b, 'Boosted Test'),\n",
    "    (train_o, 'Original Train'), (val_o, 'Original Val'), (test_o, 'Original Test')\n",
    "]\n",
    "\n",
    "for df, name in splits:\n",
    "    print(f\"\\n=========== {name} ===========\")\n",
    "    print(\"bug_report:\\n\", df['bug_report'].value_counts(normalize=True))\n",
    "    print(\"feature_request:\\n\", df['feature_request'].value_counts(normalize=True))\n",
    "    print(\"aspect:\\n\", df['aspect'].value_counts(normalize=True))\n",
    "    print(\"aspect_sentiment:\\n\", df['aspect_sentiment'].value_counts(normalize=True))\n",
    "    df.to_csv(f'{name.lower().replace(\" \", \"_\")}.csv', index=False)    # 6 seperate files are saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0e469c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch was not found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dc1bbaab74f4de3a17640dfdbe18c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "838a239984484c589a333d4b7266233b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3958943bb1e44bf816895b71fba43fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ea21d8b6a8043e5b571d260a825269e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Boosted\n",
      "  Max tokens:    471\n",
      "  Mean tokens:   49.8\n",
      "  Over 128:      180 (3.6%)\n",
      "  Over 256:      21 (0.4%)\n",
      "\n",
      "Original\n",
      "  Max tokens:    327\n",
      "  Mean tokens:   29.3\n",
      "  Over 128:      58 (1.2%)\n",
      "  Over 256:      5 (0.1%)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "for df, name in [(tagged_boosted_df, 'Boosted'), (tagged_original_df, 'Original')]:\n",
    "    lengths = df['review'].apply(lambda x: len(tokenizer.encode(x)))\n",
    "    print(f\"\\n{name}\")\n",
    "    print(f\"  Max tokens:    {lengths.max()}\")\n",
    "    print(f\"  Mean tokens:   {lengths.mean():.1f}\")\n",
    "    print(f\"  Over 128:      {(lengths > 128).sum()} ({(lengths > 128).mean():.1%})\")\n",
    "    print(f\"  Over 256:      {(lengths > 256).sum()} ({(lengths > 256).mean():.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad011e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Boosted\n",
      "  Max tokens:    471\n",
      "  Mean tokens:   49.8\n",
      "\n",
      "  Over 128: 180 (3.6%)\n",
      "    bug_report=1:      68 (37.8% of truncated)\n",
      "    feature_request=1: 17 (9.4% of truncated)\n",
      "    aspect counts:     {1: 57, 0: 47, 5: 36, 3: 26, 4: 12, 2: 2}\n",
      "    sentiment counts:  {2: 165, 0: 15}\n",
      "\n",
      "  Over 256: 21 (0.4%)\n",
      "    bug_report=1:      6 (28.6% of truncated)\n",
      "    feature_request=1: 1 (4.8% of truncated)\n",
      "    aspect counts:     {1: 6, 5: 6, 3: 5, 0: 4}\n",
      "    sentiment counts:  {2: 21}\n",
      "\n",
      "Original\n",
      "  Max tokens:    327\n",
      "  Mean tokens:   29.3\n",
      "\n",
      "  Over 128: 58 (1.2%)\n",
      "    bug_report=1:      24 (41.4% of truncated)\n",
      "    feature_request=1: 29 (50.0% of truncated)\n",
      "    aspect counts:     {0: 25, 5: 13, 1: 8, 2: 5, 4: 4, 3: 3}\n",
      "    sentiment counts:  {2: 45, 0: 10, 1: 3}\n",
      "\n",
      "  Over 256: 5 (0.1%)\n",
      "    bug_report=1:      2 (40.0% of truncated)\n",
      "    feature_request=1: 2 (40.0% of truncated)\n",
      "    aspect counts:     {0: 2, 4: 1, 1: 1, 5: 1}\n",
      "    sentiment counts:  {2: 5}\n"
     ]
    }
   ],
   "source": [
    "# Let's also see if the longer reviews are more likely to be bug reports, feature requests, or have certain aspects/sentiments\n",
    "# This finding allows us to make a better decision on the max_length for the model, which will increase the quality of the model,\n",
    "# the time to train will be longer but it is not worth removing valuable information from longer reviews which are mostly bug reports and feature requests \n",
    "# with negative sentiment (the most important ones to classify correctly)\n",
    "for df, name in [(tagged_boosted_df, 'Boosted'), (tagged_original_df, 'Original')]:\n",
    "    lengths = df['review'].apply(lambda x: len(tokenizer.encode(x)))\n",
    "    mask_128 = lengths > 128\n",
    "    mask_256 = lengths > 256\n",
    "    \n",
    "    print(f\"\\n{name}\")\n",
    "    print(f\"  Max tokens:    {lengths.max()}\")\n",
    "    print(f\"  Mean tokens:   {lengths.mean():.1f}\")\n",
    "    \n",
    "    for mask, label in [(mask_128, 'Over 128'), (mask_256, 'Over 256')]:\n",
    "        truncated = df[mask]\n",
    "        print(f\"\\n  {label}: {mask.sum()} ({mask.mean():.1%})\")\n",
    "        print(f\"    bug_report=1:      {truncated['bug_report'].sum()} ({truncated['bug_report'].mean():.1%} of truncated)\")\n",
    "        print(f\"    feature_request=1: {truncated['feature_request'].sum()} ({truncated['feature_request'].mean():.1%} of truncated)\")\n",
    "        print(f\"    aspect counts:     {truncated['aspect'].value_counts().to_dict()}\")\n",
    "        print(f\"    sentiment counts:  {truncated['aspect_sentiment'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67004c13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multitag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
